# **Building Perceptron and ADALINE from Scratch: A Practical Guide to Binary Classification**
![alt text](image.png)
## **TL;DR**

Understanding the foundations of neural networks is crucial for anyone diving into machine learning. This article explores how to implement **Perceptron** and **ADALINE** from scratch in Python using the **Breast Cancer Wisconsin dataset** for binary classification. We cover the theory behind these early neural network models, provide implementation details, and outline key learnings from training and visualizing decision boundaries.

---

## **Introduction: Why Study Perceptron and ADALINE?**

Neural networks power modern AI applications, but their foundations were laid in the **1950s and 1960s** with the development of **Perceptron** and **ADALINE**. Studying these models helps understand:

- **How single-layer neural networks work**
- **The mathematical principles behind classification**
- **Gradient descent and learning rules in neural networks**
- **The limitations of early neural networks and why deeper architectures emerged**

This article provides a step-by-step guide to implementing these models, from **theory** to **Python implementation** and **practical experimentation**.

---

## **Project Overview and Structure**

The goal of this project is to implement **Perceptron and ADALINE from scratch** for **binary classification**. The dataset used is the **Breast Cancer Wisconsin dataset**, which contains **30 numerical features** used to classify tumors as **malignant or benign**.

### **Project Structure**
The project follows a structured approach to ensure easy navigation and reproducibility.

```
ml-from-scratch/2025-03-04-perceptron-adaline/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ breast_cancer.csv  # Raw dataset
‚îÇ   ‚îú‚îÄ‚îÄ X_train_std.csv  # Standardized training data
‚îÇ   ‚îú‚îÄ‚îÄ X_test_std.csv  # Standardized test data
‚îÇ   ‚îú‚îÄ‚îÄ y_train.csv  # Training labels
‚îÇ   ‚îú‚îÄ‚îÄ y_test.csv  # Test labels
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ Perceptron_Visualization.ipynb  # Decision boundary visualization
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py  # Standardizes and preprocesses data
‚îÇ   ‚îú‚îÄ‚îÄ perceptron.py  # Perceptron class
‚îÇ   ‚îú‚îÄ‚îÄ train_perceptron.py  # Training script for Perceptron
‚îÇ   ‚îú‚îÄ‚îÄ plot_decision_boundary.py  # Visualizing Perceptron results
‚îÇ   ‚îú‚îÄ‚îÄ adaline.py  # ADALINE class
‚îÇ   ‚îú‚îÄ‚îÄ train_adaline.py  # Training script for ADALINE
‚îÇ   ‚îú‚îÄ‚îÄ plot_adaline_decision_boundary.py  # Visualizing ADALINE results
‚îÇ   ‚îú‚îÄ‚îÄ plot_adaline_loss.py  # Learning curve visualization
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ perceptron_model_2feat.npz  # Trained Perceptron model
‚îÇ   ‚îú‚îÄ‚îÄ adaline_model_2feat.npz  # Trained ADALINE model
‚îú‚îÄ‚îÄ requirements.txt  # Dependencies
‚îú‚îÄ‚îÄ README.md  # Project Documentation
```

üìå **GitHub Repository:** [ml-from-scratch - Perceptron & ADALINE](https://github.com/shanojpillai/ml-from-scratch/tree/main/2025-03-04-perceptron-adaline)

---

## **Project Implementation & Running the Project**

### **1Ô∏è‚É£ Set Up the Environment**
```bash
pip install -r requirements.txt
```

### **2Ô∏è‚É£ Preprocess the Data**
```bash
python src/data_preprocessing.py
```

### **3Ô∏è‚É£ Train the Perceptron Model**
```bash
python src/train_perceptron.py
```

### **4Ô∏è‚É£ Train the ADALINE Model**
```bash
python src/train_adaline.py
```

### **5Ô∏è‚É£ Visualize Perceptron Decision Boundary**
```bash
python src/plot_decision_boundary.py
```

### **6Ô∏è‚É£ Visualize ADALINE Learning Curve**
```bash
python src/plot_adaline_loss.py
```

## **Theory: Understanding Perceptron and ADALINE**

### **Perceptron Algorithm**

**Developed by Frank Rosenblatt in 1957**, the Perceptron is one of the earliest single-layer neural networks. It is a **linear classifier** that updates its weights based on misclassified examples.

#### **Mathematical Formulation**
For an input vector **X** = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]:

1. Compute the weighted sum:
   
   **z = (w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô) + bias**

2. Apply the **activation function (step function)**:
   
   **≈∑ = 1 if z > 0, else -1**

3. Update weights and bias for misclassified examples:
   
   **w·µ¢ ‚Üê w·µ¢ + Œ∑ (y - ≈∑) x·µ¢**
   
   **bias ‚Üê bias + Œ∑ (y - ≈∑)**
   
   where **Œ∑** is the learning rate.

#### **Limitations**
- The Perceptron can **only classify linearly separable data**.
- It does **not use gradient descent**, which limits learning efficiency.

---

### **ADALINE Algorithm**

**Developed by Bernard Widrow and Marcian Hoff in 1960**, ADALINE (Adaptive Linear Neuron) improves upon the Perceptron by using a **continuous activation function** instead of a step function.

#### **Mathematical Formulation**
1. Compute the output **≈∑** using a linear function:
   
   **≈∑ = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + bias**

2. Compute the **cost function (Mean Squared Error - MSE)**:
   
   **E = (1/2) Œ£(y - ≈∑)¬≤**

3. Use **gradient descent** to update weights:
   
   **w·µ¢ ‚Üê w·µ¢ + Œ∑ Œ£(y - ≈∑) x·µ¢**
   
   **bias ‚Üê bias + Œ∑ Œ£(y - ≈∑)**

#### **Advantages Over Perceptron**
- ADALINE uses **gradient descent**, leading to **smoother learning**.
- It **converges better** since it minimizes an explicit cost function.
- However, it **still requires linearly separable data** for effective classification.

---

## **Project Implementation & Running the Project**

### **1Ô∏è‚É£ Set Up the Environment**
```bash
pip install -r requirements.txt
```

### **2Ô∏è‚É£ Preprocess the Data**
```bash
python src/data_preprocessing.py
```

### **3Ô∏è‚É£ Train the Perceptron Model**
```bash
python src/train_perceptron.py
```

### **4Ô∏è‚É£ Train the ADALINE Model**
```bash
python src/train_adaline.py
```

### **5Ô∏è‚É£ Visualize Perceptron Decision Boundary**
```bash
python src/plot_decision_boundary.py
```

### **6Ô∏è‚É£ Visualize ADALINE Learning Curve**
```bash
python src/plot_adaline_loss.py
```

---

## **Results and Key Learnings**

1. **Perceptron only works for linearly separable data.**
2. **ADALINE converges better due to gradient descent.**
3. **Using only two features simplifies decision boundary visualization.**
4. **Real-world applications require multilayer networks for complex patterns.**

### **Future Work**
- Implement **Multilayer Perceptron (MLP)** for **non-linear decision boundaries**.
- Extend to **more complex datasets (e.g., MNIST)**.
- Experiment with **different learning rates and activation functions**.

---

## **References**

- [Breast Cancer Wisconsin Dataset - Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)
- [Perceptron Wikipedia](https://en.wikipedia.org/wiki/Perceptron)
- [ADALINE Wikipedia](https://en.wikipedia.org/wiki/Adaline)

---

## **Conclusion**

Perceptron and ADALINE lay the foundation for modern neural networks. While limited to **linear classification**, they provide valuable insights into how learning occurs in neural models. Implementing these from scratch builds intuition for **weight updates, learning rules, and activation functions**, essential for understanding **deep learning architectures**.

Would you like to explore **deep learning next?** Let‚Äôs discuss in the comments! üöÄ

